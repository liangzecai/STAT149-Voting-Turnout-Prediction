---
title: "STAT149_project_CV"
author: "Zecai Liang"
date: "4/24/2017"
output: pdf_document
---

```{r}
#### Function to Calculate Discrepency Score ####
# Input: 
        # y.true: factor vector of 'Y' and 'N'
        # y.predict: numeric vector of probabilities

# Output: disprepancy score

model_score = function(y.true, y.predict){
    
    ## convert y.true to binary of 0/1
    y.true = as.numeric(y.true) - 1
    
    ## score (consistent with Qianli's version)
    #score = -mean(y.true * log(y.predict + 10^(-23)) + (1 - y.true) * log(1 - y.predict + 10^(-23)))
    e = 1e-5
    score = -mean((y.true * log(pmax(pmin(y.predict,1-e),e)) + 
                       (1-y.true) * log(pmax(pmin(1-y.predict,1-e),e))))
    
    return(score)
}
```

For cross validation, we want to report the score on training and test data for each model:

```{r}
#### Function to Calculate Cross-Validation Score (GLM model) ####
# Input: 
        # formular: the formula of glm model
        # data
        # n_fold: k-fold cross validation, default = 10
# Output: the averaged training score and cv score

cv_score_glm = function(formula, data, n_fold = 10, seed = 0){
    set.seed(seed)
    folds_i = sample(1:n_fold, nrow(data), replace = TRUE)
    # vector to store cross validation score
    train_score = rep(0, n_fold)
    test_score = rep(0, n_fold)
    
    for (i in 1:n_fold){
        # split data
        train= data[folds_i != i, ]
        test = data[folds_i == i, ]

        # train model
        model = glm(formula, data = train, family = "binomial")
        # predict
        train.predict = predict(model, newdata = train, type = "prob")
        test.predict = predict(model, newdata = test, type = "prob")
        # evaluate score
        score1 = model_score(test$voted, test.predict[,2])
        score2 = model_score(train$voted, train.predict[,2])
        
        train_score[i] = score1
        test_score[i] = score2
    }
    
    
    return(c(mean(train_score), mean(test_score)))
}

    
```


```{r}
#### Function to Calculate Cross-Validation Score (GAM model) ####
# Input: 
        # formular: the formula of gam model (including s and)
        # data
        # n_fold: k-fold cross validation, default = 10
# Output: the averaged training score and cv score

cv_score_gam = function(formula, data, n_fold = 10, seed = 0){
    set.seed(seed)
    folds_i = sample(1:n_fold, nrow(data), replace = TRUE)
    # vector to store cross validation score
    train_score = rep(0, n_fold)
    test_score = rep(0, n_fold)
    
    for (i in 1:n_fold){
        # split data
        train= data[folds_i != i, ]
        test = data[folds_i == i, ]

        # train model
        model = gam(factor(voted) ~., data = train, family = "bionomial")
       # predict
        train.predict = predict(model, newdata = train, type = "prob")
        test.predict = predict(model, newdata = test, type = "prob")
        # evaluate score
        score1 = model_score(train$voted, train.predict[,2])
        score2 = model_score(test$voted, test.predict[,2])
        
        train_score[i] = score1
        test_score[i] = score2
    }
    
    
    return(c(mean(train_score), mean(test_score)))
}

```


```{r}
#### Function to Calculate Cross-Validation Score (RF model) ####
# Input: 
        # ntree: parameter 'ntree' for randomForest
        # mtry: parameter 'mtry' for randomForest
        # data
        # n_fold: k-fold cross validation, default = 10
# Output: the averaged training score and cv score

cv_score_rf = function(ntree, mtry, data, nodesize, n_fold = 10, seed = 0){
    set.seed(seed)
    folds_i = sample(1:n_fold, nrow(data), replace = TRUE)
    # vector to store cross validation score
    train_score = rep(0, n_fold)
    test_score = rep(0, n_fold)
    
    for (i in 1:n_fold){
        # split data
        train= data[folds_i != i, ]
        test = data[folds_i == i, ]

        # train model
        model = randomForest(factor(voted) ~., data = train, 
                             ntree = ntree, mtry = mtry, nodesize = nodesize)
       # predict
        train.predict = predict(model, newdata = train, type = "prob")
        test.predict = predict(model, newdata = test, type = "prob")
        # evaluate score
        score1 = model_score(train$voted, train.predict[,2])
        score2 = model_score(test$voted, test.predict[,2])
        
        train_score[i] = score1
        test_score[i] = score2
    }
    
    
    return(c(mean(train_score), mean(test_score)))
}
```


